{
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8486285,
          "sourceType": "datasetVersion",
          "datasetId": 5062385
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafabayrak0/Imagenius/blob/main/comvis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "w7HBLxVvW3Uz",
        "execution": {
          "iopub.status.busy": "2024-05-22T12:37:34.352494Z",
          "iopub.execute_input": "2024-05-22T12:37:34.353321Z",
          "iopub.status.idle": "2024-05-22T12:37:34.359803Z",
          "shell.execute_reply.started": "2024-05-22T12:37:34.353285Z",
          "shell.execute_reply": "2024-05-22T12:37:34.358918Z"
        },
        "trusted": true
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the working directory to the project folder\n",
        "project_path = '/content/drive/MyDrive/Main'\n",
        "os.chdir(project_path)\n",
        "\n",
        "# Add the project directory to the Python path\n",
        "sys.path.append(project_path)\n",
        "\n",
        "# Assuming config is defined somewhere, replace this with actual paths\n",
        "data_tra_path = '/content/drive/MyDrive/Main/DeepCrack/train'  # Replace with your actual path\n",
        "data_val_path = '/content/drive/MyDrive/Main/DeepCrack/test'    # Replace with your actual path\n",
        "\n",
        "DIR_IMG_tra = os.path.join(data_tra_path, 'images')\n",
        "DIR_MASK_tra = os.path.join(data_tra_path, 'masks')\n",
        "DIR_IMG_val = os.path.join(data_val_path, 'images')\n",
        "DIR_MASK_val = os.path.join(data_val_path, 'masks')\n",
        "\n",
        "img_names_tra = [path.name for path in Path(DIR_IMG_tra).glob('*.jpg')]\n",
        "mask_names_tra = [path.name for path in Path(DIR_MASK_tra).glob('*.png')]\n",
        "img_names_val = [path.name for path in Path(DIR_IMG_val).glob('*.jpg')]\n",
        "mask_names_val = [path.name for path in Path(DIR_MASK_val).glob('*.png')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAfLWXZ5NuBz",
        "outputId": "0e514bc4-ae2c-423a-d5cd-ecd4e684903a",
        "execution": {
          "iopub.status.busy": "2024-05-22T12:37:34.361577Z",
          "iopub.execute_input": "2024-05-22T12:37:34.361868Z",
          "iopub.status.idle": "2024-05-22T12:37:34.374788Z",
          "shell.execute_reply.started": "2024-05-22T12:37:34.361844Z",
          "shell.execute_reply": "2024-05-22T12:37:34.373952Z"
        },
        "trusted": true
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_and_masks(img_dir, mask_dir):\n",
        "    img_names = sorted(os.listdir(img_dir))  # Ensure the list is sorted\n",
        "    mask_names = sorted(os.listdir(mask_dir))  # Ensure the list is sorted\n",
        "\n",
        "    images = []\n",
        "    masks = []\n",
        "    i = 0\n",
        "    for img_name, mask_name in zip(img_names, mask_names):\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "        mask_path = os.path.join(mask_dir, mask_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
        "\n",
        "        images.append(np.array(image))\n",
        "        masks.append(np.array(mask))\n",
        "        i += 1\n",
        "        if i == 10:\n",
        "            break\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "# Directories\n",
        "DIR_IMG_tra = '/content/drive/MyDrive/Main/DeepCrack/train/images'\n",
        "DIR_MASK_tra = '/content/drive/MyDrive/Main/DeepCrack/train/masks'\n",
        "DIR_IMG_val = '/content/drive/MyDrive/Main/DeepCrack/test/images'\n",
        "DIR_MASK_val = '/content/drive/MyDrive/Main/DeepCrack/test/masks'\n",
        "\n",
        "# Load training and validation images and masks\n",
        "train_images, train_masks = load_images_and_masks(DIR_IMG_tra, DIR_MASK_tra)\n",
        "val_images, val_masks = load_images_and_masks(DIR_IMG_val, DIR_MASK_val)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(f'Loaded {len(train_images)} training images and {len(train_masks)} training masks.')\n",
        "print(f'Loaded {len(val_images)} validation images and {len(val_masks)} validation masks.')"
      ],
      "metadata": {
        "id": "9YOiUU4xYs5s",
        "execution": {
          "iopub.status.busy": "2024-05-22T12:37:34.399552Z",
          "iopub.execute_input": "2024-05-22T12:37:34.400084Z",
          "iopub.status.idle": "2024-05-22T12:37:37.224274Z",
          "shell.execute_reply.started": "2024-05-22T12:37:34.400051Z",
          "shell.execute_reply": "2024-05-22T12:37:37.223324Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4cd0e2-c251-4a1e-fc55-d653b95b21b3"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 training images and 10 training masks.\n",
            "Loaded 10 validation images and 10 validation masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Verify image and mask pairs to ensure they are correctly aligned\n",
        "def verify_image_mask_pairs(images, masks, num_pairs=5):\n",
        "    fig, axs = plt.subplots(num_pairs, 2, figsize=(10, 5 * num_pairs))\n",
        "\n",
        "    for i in range(num_pairs):\n",
        "        img = images[i]\n",
        "        mask = masks[i]\n",
        "\n",
        "        axs[i, 0].imshow(img)\n",
        "        axs[i, 0].set_title('Image')\n",
        "        axs[i, 0].axis('off')\n",
        "\n",
        "        axs[i, 1].imshow(mask, cmap='gray')\n",
        "        axs[i, 1].set_title('Mask')\n",
        "        axs[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Verify the first few pairs of images and masks\n",
        "verify_image_mask_pairs(train_images, train_masks, num_pairs=5)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "GXlp0pDtiQR2",
        "outputId": "947468c9-1551-44c9-fb05-7a14bb7d1aa3"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Verify image and mask pairs to ensure they are correctly aligned\\ndef verify_image_mask_pairs(images, masks, num_pairs=5):\\n    fig, axs = plt.subplots(num_pairs, 2, figsize=(10, 5 * num_pairs))\\n    \\n    for i in range(num_pairs):\\n        img = images[i]\\n        mask = masks[i]\\n        \\n        axs[i, 0].imshow(img)\\n        axs[i, 0].set_title('Image')\\n        axs[i, 0].axis('off')\\n        \\n        axs[i, 1].imshow(mask, cmap='gray')\\n        axs[i, 1].set_title('Mask')\\n        axs[i, 1].axis('off')\\n    \\n    plt.tight_layout()\\n    plt.show()\\n\\n# Verify the first few pairs of images and masks\\nverify_image_mask_pairs(train_images, train_masks, num_pairs=5)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchCrackSegmentationDataset(Dataset):\n",
        "    def __init__(self, images, masks, patch_size=(960, 540), transform=None):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.patch_size = patch_size\n",
        "        self.transform = transform\n",
        "\n",
        "        # Precompute all patches\n",
        "        self.image_patches = []\n",
        "        self.mask_patches = []\n",
        "        for image, mask in zip(self.images, self.masks):\n",
        "            img_patches, msk_patches = self.divide_image_into_patches(image, mask, patch_size)\n",
        "            self.image_patches.extend(img_patches)\n",
        "            self.mask_patches.extend(msk_patches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_patches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_patch = self.image_patches[idx]\n",
        "        mask_patch = self.mask_patches[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image_patch = self.transform(image_patch)\n",
        "            mask_patch = torch.tensor(mask_patch, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return image_patch, mask_patch\n",
        "\n",
        "    def pad_image(self, image, patch_size):\n",
        "        h, w = image.shape[:2]\n",
        "        patch_h, patch_w = patch_size\n",
        "\n",
        "        # Calculate the padding needed\n",
        "        pad_h = (patch_h - h % patch_h) % patch_h\n",
        "        pad_w = (patch_w - w % patch_w) % patch_w\n",
        "\n",
        "        if len(image.shape) == 3:\n",
        "            # For images with channels\n",
        "            padded_image = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
        "        else:\n",
        "            # For masks without channels\n",
        "            padded_image = np.pad(image, ((0, pad_h), (0, pad_w)), mode='constant')\n",
        "\n",
        "        return padded_image\n",
        "\n",
        "    def divide_image_into_patches(self, image, mask, patch_size=(960, 540)):\n",
        "        image = self.pad_image(image, patch_size)\n",
        "        mask = self.pad_image(mask, patch_size)\n",
        "\n",
        "        image_patches = []\n",
        "        mask_patches = []\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        patch_h, patch_w = patch_size\n",
        "\n",
        "        for i in range(0, h, patch_h):\n",
        "            for j in range(0, w, patch_w):\n",
        "                image_patch = image[i:i+patch_h, j:j+patch_w]\n",
        "                mask_patch = mask[i:i+patch_h, j:j+patch_w]\n",
        "\n",
        "                image_patches.append(image_patch)\n",
        "                mask_patches.append(mask_patch)\n",
        "\n",
        "        return image_patches, mask_patches\n",
        "\n",
        "# Define transformations (if any)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create dataset instances for training and validation data using patches\n",
        "train_patch_dataset = PatchCrackSegmentationDataset(train_images, train_masks, transform=transform)\n",
        "val_patch_dataset = PatchCrackSegmentationDataset(val_images, val_masks, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "patch_batch_size = 2  # Adjust batch size as needed\n",
        "\n",
        "train_patch_loader = DataLoader(train_patch_dataset, batch_size=patch_batch_size, shuffle=True)\n",
        "val_patch_loader = DataLoader(val_patch_dataset, batch_size=patch_batch_size, shuffle=False)\n",
        "\n",
        "# Print the number of batches\n",
        "print(f'Number of training batches with patches: {len(train_patch_loader)}')\n",
        "print(f'Number of validation batches with patches: {len(val_patch_loader)}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-22T12:37:37.268095Z",
          "iopub.execute_input": "2024-05-22T12:37:37.268428Z",
          "iopub.status.idle": "2024-05-22T12:37:37.283240Z",
          "shell.execute_reply.started": "2024-05-22T12:37:37.268396Z",
          "shell.execute_reply": "2024-05-22T12:37:37.282340Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjwfTdDGUD3K",
        "outputId": "067c9a48-1891-48e3-85ee-f8b673781e5c"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training batches with patches: 10\n",
            "Number of validation batches with patches: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition ensuring raw logits output\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-22T12:37:37.284263Z",
          "iopub.execute_input": "2024-05-22T12:37:37.284573Z",
          "iopub.status.idle": "2024-05-22T12:37:37.298155Z",
          "shell.execute_reply.started": "2024-05-22T12:37:37.284534Z",
          "shell.execute_reply": "2024-05-22T12:37:37.297188Z"
        },
        "trusted": true,
        "id": "vsJDoPgMUD3L"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_level_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the pixel-level accuracy between the ground truth mask and the predicted mask.\n",
        "\n",
        "    Args:\n",
        "    y_true (numpy.ndarray): Ground truth mask.\n",
        "    y_pred (numpy.ndarray): Predicted mask.\n",
        "\n",
        "    Returns:\n",
        "    float: Pixel-level accuracy.\n",
        "    \"\"\"\n",
        "    assert y_true.shape == y_pred.shape, \"The shape of the ground truth and predicted masks must match.\"\n",
        "\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "8LqhvJqqduQY"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_patch_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, masks in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    return model, train_losses\n",
        "\n",
        "# Evaluation function with pixel-level accuracy\n",
        "def evaluate_patch_model_with_accuracy(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, masks in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "            outputs = (outputs > 0.5).float()\n",
        "            outputs = outputs.cpu().numpy()\n",
        "            masks = masks.cpu().numpy()\n",
        "\n",
        "            for i in range(len(outputs)):\n",
        "                accuracy = pixel_level_accuracy(masks[i], outputs[i])\n",
        "                total_accuracy += accuracy\n",
        "                num_samples += 1\n",
        "\n",
        "    val_loss /= num_samples\n",
        "    avg_accuracy = total_accuracy / num_samples\n",
        "    print(f'Validation Loss: {val_loss:.4f}')\n",
        "    print(f'Average Pixel-level Accuracy: {avg_accuracy:.4f}')\n",
        "    return val_loss, avg_accuracy\n",
        "\n",
        "model = SimpleCNN()\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model with patches and capture loss history\n",
        "num_epochs = 5\n",
        "trained_patch_model, train_losses = train_patch_model(model, train_patch_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "# Evaluate the model with patches and capture validation loss and accuracy\n",
        "val_loss, avg_accuracy = evaluate_patch_model_with_accuracy(trained_patch_model, val_patch_loader, criterion)\n",
        "\n"
      ],
      "metadata": {
        "id": "90NpvuWDYlgz",
        "execution": {
          "iopub.status.busy": "2024-05-22T12:37:38.069704Z",
          "iopub.execute_input": "2024-05-22T12:37:38.070175Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "08c46c52-a935-4bdc-856b-85f58a27bbd2"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target size (torch.Size([2, 1, 960, 540])) must be the same as input size (torch.Size([2, 1, 953, 529]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-199-85a00776b34e>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Train the model with patches and capture loss history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrained_patch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_patch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_patch_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Evaluate the model with patches and capture validation loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-199-85a00776b34e>\u001b[0m in \u001b[0;36mtrain_patch_model\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    732\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3224\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([2, 1, 960, 540])) must be the same as input size (torch.Size([2, 1, 953, 529]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss\n",
        "def plot_loss_curves(train_losses, val_loss):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_losses, label='Training Loss')\n",
        "    plt.axhline(y=val_loss, color='r', linestyle='-', label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the loss curves\n",
        "plot_loss_curves(train_losses, val_loss)\n"
      ],
      "metadata": {
        "id": "reSvLZFFZNrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize first image in the train data\n",
        "image, mask = train_patch_dataset[0]\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title('Image')\n",
        "plt.show()\n",
        "plt.imshow(mask.squeeze(), cmap='gray')\n",
        "plt.title('Mask')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cQE5nHEilKvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first image's resolution values\n",
        "image, mask = train_patch_dataset[0]\n",
        "print(f'Image resolution: {image.shape}')\n",
        "print(f'Mask resolution: {mask.shape}')"
      ],
      "metadata": {
        "id": "0xB-FwwxlUuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict and visualize samples\n",
        "def predict_and_visualize(model, dataset, num_samples=5):\n",
        "    model.eval()\n",
        "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "    fig, axs = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        image_patch, mask_patch = dataset[idx]\n",
        "        with torch.no_grad():\n",
        "            output = model(image_patch.unsqueeze(0)).squeeze(0)\n",
        "            output = torch.sigmoid(output)\n",
        "            output = (output > 0.5).float()\n",
        "\n",
        "        axs[i, 0].imshow(image_patch.permute(1, 2, 0))\n",
        "        axs[i, 0].set_title('Input Image')\n",
        "        axs[i, 1].imshow(mask_patch.squeeze(), cmap='gray')\n",
        "        axs[i, 1].set_title('Ground Truth Mask')\n",
        "        axs[i, 2].imshow(output.squeeze(), cmap='gray')\n",
        "        axs[i, 2].set_title('Predicted Mask')\n",
        "\n",
        "        for ax in axs[i]:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Predict and visualize samples from the validation dataset\n",
        "predict_and_visualize(trained_patch_model, val_patch_dataset, num_samples=5)\n"
      ],
      "metadata": {
        "id": "4wi6cyqsZP0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}