{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafabayrak0/Imagenius/blob/main/Copy_of_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the working directory to the project folder\n",
        "project_path = '/content/drive/MyDrive/Main'\n",
        "os.chdir(project_path)\n",
        "\n",
        "# Add the project directory to the Python path\n",
        "sys.path.append(project_path)"
      ],
      "metadata": {
        "id": "qfOS1gOO7YHb",
        "outputId": "0c43c370-6303-4b92-a637-e68eea13d5ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ptflops"
      ],
      "metadata": {
        "id": "qGOdAQQr8CnI",
        "outputId": "745cadf5-9cde-4d82-dcf6-2f84cf28ef7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->ptflops)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->ptflops)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->ptflops)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->ptflops)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->ptflops)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->ptflops)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->ptflops)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->ptflops)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->ptflops)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->ptflops)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->ptflops)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->ptflops)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ptflops\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 ptflops-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHtPOaPC6rkp"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from utils.utils import *\n",
        "from pathlib import Path\n",
        "from newloader import Crack_loader\n",
        "from model.TransMUNet import TransMUNet\n",
        "from torch.utils.data import DataLoader\n",
        "from ptflops import get_model_complexity_info\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "setup_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUkBxTBk6rkt"
      },
      "outputs": [],
      "source": [
        "number_classes = int(config['number_classes'])\n",
        "input_channels = 3\n",
        "best_val_loss  = np.inf\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "data_tra_path = '/content/drive/MyDrive/Main/DeepCrack/train'\n",
        "data_val_path = '/content/drive/MyDrive/Main/DeepCrack/test'\n",
        "\n",
        "DIR_IMG_tra  = os.path.join(data_tra_path, 'images')\n",
        "DIR_MASK_tra = os.path.join(data_tra_path, 'masks')\n",
        "\n",
        "DIR_IMG_val  = os.path.join(data_val_path, 'images')\n",
        "DIR_MASK_val = os.path.join(data_val_path, 'masks')\n",
        "\n",
        "img_names_tra  = [path.name for path in Path(DIR_IMG_tra).glob('*.jpg')]\n",
        "mask_names_tra = [path.name for path in Path(DIR_MASK_tra).glob('*.png')]\n",
        "\n",
        "img_names_val  = [path.name for path in Path(DIR_IMG_val).glob('*.jpg')]\n",
        "mask_names_val = [path.name for path in Path(DIR_MASK_val).glob('*.png')]\n",
        "\n",
        "train_dataset = Crack_loader(img_dir=DIR_IMG_tra, img_fnames=img_names_tra, mask_dir=DIR_MASK_tra, mask_fnames=mask_names_tra, isTrain=True)\n",
        "valid_dataset = Crack_loader(img_dir=DIR_IMG_val, img_fnames=img_names_val, mask_dir=DIR_MASK_val, mask_fnames=mask_names_val, resize=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_tra_path"
      ],
      "metadata": {
        "id": "ytjHcgC68fYy",
        "outputId": "c1317045-bb11-447e-86dd-8bf4d9d27bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/DeepCrack/train'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGPEYXkP6rku",
        "outputId": "54419cce-714b-410f-c094-1d44f4fda743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset:300\n",
            "validation_dataset:237\n"
          ]
        }
      ],
      "source": [
        "print(f'train_dataset:{len(train_dataset)}')\n",
        "print(f'validation_dataset:{len(valid_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaSMlddD6rkv"
      },
      "outputs": [],
      "source": [
        "# To work with small dataset, take first 10 samples\n",
        "train_dataset = torch.utils.data.Subset(train_dataset, range(20))\n",
        "valid_dataset = torch.utils.data.Subset(valid_dataset, range(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smIX8hsq6rkv",
        "outputId": "5435d3aa-a88e-4bec-9434-d537dc1d9d35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset:20\n",
            "valiant_dataset:20\n"
          ]
        }
      ],
      "source": [
        "print(f'train_dataset:{len(train_dataset)}')\n",
        "print(f'valiant_dataset:{len(valid_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSvJ6Yj66rkv",
        "outputId": "0afa5cc7-4721-404b-b3ed-a0b85b487bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flops:  112.49 GMac params:  2.03 M\n"
          ]
        }
      ],
      "source": [
        "train_loader  = DataLoader(train_dataset, batch_size = int(config['batch_size_tr']), shuffle= True,  drop_last=True)\n",
        "val_loader    = DataLoader(valid_dataset, batch_size = int(config['batch_size_va']), shuffle= False, drop_last=True)\n",
        "\n",
        "# Net = TransMUNet(n_classes = number_classes)\n",
        "Net = TransMUNet(in_channels=input_channels, out_channels=number_classes, nhead=int(config['nhead']), num_layers=int(config['num_layers']))\n",
        "flops, params = get_model_complexity_info(Net, (3, 256, 256), as_strings=True, print_per_layer_stat=False)\n",
        "print('flops: ', flops, 'params: ', params)\n",
        "message = 'flops:%s, params:%s' % (flops, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHTy3hUh6rkv"
      },
      "outputs": [],
      "source": [
        "Net = Net.to(device)\n",
        "# load pretrained model\n",
        "if int(config['pretrained']):\n",
        "    Net.load_state_dict(torch.load(config['saved_model'], map_location='cpu')['model_weights'])\n",
        "    best_val_loss = torch.load(config['saved_model'], map_location='cpu')['val_loss']\n",
        "\n",
        "optimizer = optim.Adam(Net.parameters(), lr= float(config['lr']))\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor = 0.5, patience = config['patience'])\n",
        "criteria  = DiceBCELoss()\n",
        "\n",
        "# visual\n",
        "visualizer = Visualizer(isTrain=True)\n",
        "log_name = os.path.join('./checkpoints', config['loss_filename'])\n",
        "with open(log_name, \"a\") as log_file:\n",
        "            log_file.write('%s\\n' % message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpjUPex46rkw"
      },
      "outputs": [],
      "source": [
        "for ep in range(int(config['epochs'])):\n",
        "    # # train\n",
        "    Net.train()\n",
        "    epoch_loss = 0\n",
        "    for itter, batch in enumerate(train_loader):\n",
        "        img = batch['image'].to(device, dtype=torch.float)\n",
        "        msk = batch['mask'].to(device)\n",
        "        boundary = batch['boundary'].to(device)\n",
        "        mask_type = torch.float32 if Net.n_classes == 1 else torch.long\n",
        "        msk = msk.to(device=device, dtype=mask_type)\n",
        "        boundary = boundary.to(device=device, dtype=mask_type)\n",
        "        msk_pred, B = Net(img,istrain=True)\n",
        "        # Ensure msk has the correct dimensions\n",
        "        if msk.shape[1] == 1:\n",
        "            msk = msk.squeeze(1)  # Remove channel dimension if it's 1\n",
        "\n",
        "        # Print shapes of predictions and ground truth for debugging\n",
        "        print(f\"Shape of msk_pred: {msk_pred.shape}\")\n",
        "        print(f\"Shape of msk: {msk.shape}\")\n",
        "\n",
        "    #     loss = criteria(msk_pred, msk)\n",
        "    #     loss_boundary = criteria(B, boundary)\n",
        "    #     tloss    = (0.8*(loss)) + (0.2*loss_boundary)\n",
        "    #     optimizer.zero_grad()\n",
        "    #     tloss.backward()\n",
        "    #     epoch_loss += tloss.item()\n",
        "    #     optimizer.step()\n",
        "    #     if (itter+1)%int(float(config['progress_p']) * len(train_loader)) == 0:\n",
        "    #         lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "    #         print(f' Epoch>> {ep+1} and itteration {itter+1} loss>>{epoch_loss/(itter+1)}')\n",
        "    #     if (itter+1)*int(config['batch_size_tr']) == len(train_dataset):\n",
        "    #         visualizer.print_current_losses(epoch=(ep+1), iters=(itter+1), loss=((epoch_loss/(itter+1))), lr=lr, isVal=False)\n",
        "\n",
        "\n",
        "    # eval\n",
        "    with torch.no_grad():\n",
        "        print('val_mode')\n",
        "        val_loss = 0\n",
        "        Net.eval()\n",
        "        for itter, batch in enumerate(val_loader):\n",
        "            img = batch['image'].to(device, dtype=torch.float)\n",
        "            msk = batch['mask'].to(device)\n",
        "            mask_type = torch.float32 if Net.n_classes == 1 else torch.long\n",
        "            msk = msk.to(device=device, dtype=mask_type)\n",
        "            msk_pred, B = Net(img)\n",
        "            if msk.shape[1] == 1:\n",
        "                msk = msk.squeeze(1)  # Remove channel dimension if it's 1\n",
        "            # Print shapes of predictions and ground truth\n",
        "            # print(f\"Shape of msk_pred: {msk_pred.shape}\")\n",
        "            # print(f\"Shape of msk: {msk.shape}\")\n",
        "            loss = criteria(msk_pred, msk)\n",
        "            val_loss += loss.item()\n",
        "        visualizer.print_current_losses(epoch=ep+1, loss=(abs(val_loss/(itter+1))), isVal=True)\n",
        "        mean_val_loss = (val_loss/(itter+1))\n",
        "        # Check the performance and save the model\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            best = ep + 1\n",
        "            best_val_loss = copy.deepcopy(mean_val_loss)\n",
        "            print('New best loss, saving...,best_val_loss=%6f' % (best_val_loss))\n",
        "            with open(log_name, \"a\") as log_file:\n",
        "                message = 'New best loss, saving...,best_val_loss=%6f' % (best_val_loss)\n",
        "                log_file.write('%s\\n' % message)\n",
        "            state = copy.deepcopy({'model_weights': Net.state_dict(), 'val_loss': best_val_loss})\n",
        "            torch.save(state, config['saved_model'])\n",
        "\n",
        "    scheduler.step(mean_val_loss)\n",
        "\n",
        "    if ep+1 == int(config['epochs']):\n",
        "        visualizer.print_end(best, best_val_loss)\n",
        "        state = copy.deepcopy({'model_weights': Net.state_dict(), 'val_loss': best_val_loss})\n",
        "        torch.save(state, config['saved_model_final'])\n",
        "\n",
        "print('Trainig phase finished')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cv_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}